{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32fcb737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import networkx as nx\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "from causal_reasoning.causal_model import CausalModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5784e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, inter_label: str, inter_value: int, target_label: str, target_value: int, query_value: float) -> None:\n",
    "        self.intervention_label = inter_label\n",
    "        self.intervention_value = inter_value\n",
    "        self.target_label = target_label\n",
    "        self.target_value = target_value\n",
    "        self.query = query_value\n",
    "\n",
    "def run_queries(nodes_set: set, model: CausalModel, empirical_probs, output_path: str):\n",
    "    with open(output_path, 'w') as file:\n",
    "        queries = []\n",
    "        for intervention in nodes_set:\n",
    "            for target in nodes_set:\n",
    "                if intervention == target:\n",
    "                    continue\n",
    "                intervention_value = 0\n",
    "                target_value = 0\n",
    "                model.set_interventions([(intervention, intervention_value)])\n",
    "                model.set_target((target, target_value))\n",
    "                start = time.perf_counter()\n",
    "                pn = model.inference_intervention_query()\n",
    "                end = time.perf_counter()\n",
    "                pn_time = end-start\n",
    "                queries.append(Query(intervention, intervention_value, target, target_value, pn))\n",
    "\n",
    "\n",
    "                intervention_value = 1\n",
    "                target_value = 1\n",
    "                model.set_interventions([(intervention, intervention_value)])\n",
    "                model.set_target((target, target_value))\n",
    "                start = time.perf_counter()\n",
    "                ps = model.inference_intervention_query()\n",
    "                end = time.perf_counter()\n",
    "                ps_time = end-start\n",
    "                queries.append(Query(intervention, intervention_value, target, target_value, ps))\n",
    "                \n",
    "                # file.write(f\"PS = P({target}={target_value}|do({intervention}={intervention_value}))\\n\")\n",
    "                file.write(f\"PS = P({target}={1}|do({intervention}={1}))\\n\")\n",
    "                file.write(f\"{ps[0]} <= PS <= {ps[1]}\\n\")\n",
    "                file.write(f\"PS Inference time taken: {ps_time:.6f} seconds\\n\")\n",
    "                file.write(\"----\\n\")\n",
    "                file.write(f\"PN = P({target}={0}|do({intervention}={0}))\\n\")\n",
    "                file.write(f\"{pn[0]} <= PN <= {pn[1]}\\n\")\n",
    "                file.write(f\"PN Inference time taken: {pn_time:.6f} seconds\\n\")\n",
    "                file.write(\"----\\n\")\n",
    "\n",
    "                empirical_ps = empirical_probs[intervention][1][target]\n",
    "                empirical_pn = empirical_probs[intervention][0][target]\n",
    "                \n",
    "                if ps[0] == 'None' or ps[0] is None or ps[1] == 'None' or ps[1] is None or pn[0] == 'None' or pn[0] is None or pn[1] == 'None' or pn[1] is None: \n",
    "                    file.write(f\"PNS = P({target}|do({intervention}))\\n\")\n",
    "                    file.write(f\"PNS == None\\n\")\n",
    "                    file.write(\"---------------------------------------------------------\\n\")\n",
    "                    continue\n",
    "\n",
    "                ps_lower = empirical_ps*float(ps[0])\n",
    "                ps_upper = empirical_ps*float(ps[1])\n",
    "                pn_lower = empirical_pn*float(pn[0])\n",
    "                pn_upper = empirical_pn*float(pn[1])\n",
    "\n",
    "                pns = [0,0]\n",
    "                pns[0] = min(ps_lower+pn_lower, ps_lower+pn_upper)\n",
    "                pns[1] = max(ps_upper+pn_lower, ps_upper+pn_upper)\n",
    "                file.write(f\"PNS = P({target}|do({intervention}))\\n\")\n",
    "                file.write(f\"{pn[0]} <= PNS <= {pn[1]}\\n\")\n",
    "                file.write(\"---------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "def run_pn_ps(nodes_set: set, model: CausalModel, target: str, output_path: str):\n",
    "    pn_ps = {}\n",
    "    for intervention in nodes_set:\n",
    "        if intervention == target:\n",
    "            continue\n",
    "        pn_ps[intervention] = {}\n",
    "\n",
    "        intervention_value = 0\n",
    "        target_value = 0\n",
    "        model.set_interventions([(intervention, intervention_value)])\n",
    "        model.set_target((target, target_value))\n",
    "        start = time.perf_counter()\n",
    "        pn = model.inference_intervention_query()\n",
    "        end = time.perf_counter()\n",
    "        pn_time = end-start\n",
    "        pn_ps[intervention][\"PN\"] = pn\n",
    "\n",
    "        intervention_value = 1\n",
    "        target_value = 1\n",
    "        model.set_interventions([(intervention, intervention_value)])\n",
    "        model.set_target((target, target_value))\n",
    "        start = time.perf_counter()\n",
    "        ps = model.inference_intervention_query()\n",
    "        end = time.perf_counter()\n",
    "        ps_time = end-start\n",
    "        pn_ps[intervention][\"PS\"] = ps\n",
    "            \n",
    "    with open(f'{output_path}.json', 'w') as f:\n",
    "        json.dump(pn_ps, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa746d00",
   "metadata": {},
   "source": [
    "## First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca736e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_list_1 = [\n",
    "    ('NewDeploy', 'MemoryLeak'),\n",
    "    ('MemoryLeak', 'MemoryUsageHigh'), \n",
    "    (\"MemoryUsageHigh\", \"ServiceCrash\"), \n",
    "    (\"ServiceCrash\", \"OutageIncident\"),\n",
    "    (\"HeavyTraffic\", \"MemoryUsageHigh\"), \n",
    "    (\"HeavyTraffic\", \"ServiceCrash\"),\n",
    "    # UNOBS\n",
    "    ('Unob_helper_1', 'MemoryLeak'),\n",
    "    ('Unob_helper_2', 'OutageIncident'),\n",
    "    ('Unob_helper_3', 'NewDeploy'),\n",
    "]\n",
    "latent_nodes_1 = ['HeavyTraffic', 'Unob_helper_1', 'Unob_helper_2', 'Unob_helper_3']\n",
    "nodes_set_1 = set()\n",
    "for tuple in edges_list_1:\n",
    "    if tuple[0] not in latent_nodes_1:\n",
    "        nodes_set_1.add(tuple[0])\n",
    "    if tuple[1] not in latent_nodes_1:\n",
    "        nodes_set_1.add(tuple[1])\n",
    "\n",
    "edges_1 = nx.DiGraph(edges_list_1)\n",
    "\n",
    "df_small_scale_model = pd.read_csv(\"small_scale_outage_incident_seed42.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b3fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_probs_model_1 = {}\n",
    "for intervention in nodes_set_1:\n",
    "    empirical_probs_model_1[intervention] = [{},{}]\n",
    "    for target in nodes_set_1:\n",
    "        if target == intervention:\n",
    "            continue\n",
    "        empirical_probs_model_1[intervention][0][target] = df_small_scale_model[(df_small_scale_model[intervention] == 0) & (df_small_scale_model[target] == 0)].shape[0]\n",
    "        empirical_probs_model_1[intervention][1][target] = df_small_scale_model[(df_small_scale_model[intervention] == 1) & (df_small_scale_model[target] == 1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bad013",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = CausalModel(\n",
    "    data=df_small_scale_model,\n",
    "    edges=edges_1,\n",
    "    unobservables_labels=latent_nodes_1,\n",
    ")\n",
    "# run_queries(nodes_set_1, model_1, empirical_probs_model_1, \"output_small_scale_outage_incident_seed42.txt\")\n",
    "# run_pn_ps(nodes_set_1, model_1, \"OutageIncident\", \"output_small_scale_outage_incident_seed42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45738880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a824ea8",
   "metadata": {},
   "source": [
    "## Second Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_list_2 = [\n",
    "    (\"DB_Change\", \"DB_Latency\"),\n",
    "    (\"DB_Latency\", \"MS-B_Latency\"), \n",
    "    (\"MS-B_Latency\", \"MS-B_Error\"), \n",
    "    (\"MS-B_Latency\", \"MS-A_Latency\"),\n",
    "    (\"MS-B_Error\", \"MS-A_Error\"), \n",
    "    (\"MS-A_Latency\", \"MS-A_Threads\"), \n",
    "    (\"MS-A_Threads\", \"MS-A_Crash\"), \n",
    "    (\"MS-A_Error\", \"Outage\"), \n",
    "    (\"MS-A_Crash\", \"Outage\"), \n",
    "    (\"HeavyTraffic\", \"DB_Latency\"), \n",
    "    (\"HeavyTraffic\", \"MS-A_Latency\"),\n",
    "    # UNOBS\n",
    "    ('Unob_helper_1', 'DB_Change'),\n",
    "    ('Unob_helper_2', 'MS-B_Latency'),\n",
    "    ('Unob_helper_3', 'MS-B_Error'),\n",
    "    ('Unob_helper_4', 'MS-A_Error'),\n",
    "    ('Unob_helper_5', 'MS-A_Threads'),\n",
    "    ('Unob_helper_6', 'MS-A_Crash'),\n",
    "    ('Unob_helper_7', 'Outage'),\n",
    "]\n",
    "\n",
    "latent_nodes_2 = ['HeavyTraffic', 'Unob_helper_1', 'Unob_helper_2', 'Unob_helper_3', 'Unob_helper_4', 'Unob_helper_5', 'Unob_helper_6', 'Unob_helper_7']\n",
    "nodes_set_2 = set()\n",
    "for tuple in edges_list_2:\n",
    "    if tuple[0] not in latent_nodes_2:\n",
    "        nodes_set_2.add(tuple[0])\n",
    "    if tuple[1] not in latent_nodes_2:\n",
    "        nodes_set_2.add(tuple[1])\n",
    "\n",
    "edges_2 = nx.DiGraph(edges_list_2)\n",
    "\n",
    "df_medium_scale_incident = pd.read_csv(\"medium_scale_outage_incident_seed42.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_probs_model_2 = {}\n",
    "for intervention in nodes_set_2:\n",
    "    empirical_probs_model_2[intervention] = [{},{}]\n",
    "    for target in nodes_set_2:\n",
    "        if target == intervention:\n",
    "            continue\n",
    "        empirical_probs_model_2[intervention][0][target] = df_medium_scale_incident[(df_medium_scale_incident[intervention] == 0) & (df_medium_scale_incident[target] == 0)].shape[0]\n",
    "        empirical_probs_model_2[intervention][1][target] = df_medium_scale_incident[(df_medium_scale_incident[intervention] == 1) & (df_medium_scale_incident[target] == 1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = CausalModel(\n",
    "    data=df_medium_scale_incident,\n",
    "    edges=edges_2,\n",
    "    unobservables_labels=latent_nodes_2,\n",
    ")\n",
    "# run_queries(nodes_set_2, model_2, empirical_probs_model_2, \"output_medium_scale_outage_incident_seed42.txt\")\n",
    "# run_pn_ps(nodes_set_2, model_2, \"Outage\", \"output_medium_scale_outage_incident_seed42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b0dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dowhy\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "edges_list_2 = [\n",
    "    (\"DB_Change\", \"DB_Latency\"),\n",
    "    (\"DB_Latency\", \"MS-B_Latency\"), \n",
    "    (\"MS-B_Latency\", \"MS-B_Error\"), \n",
    "    (\"MS-B_Latency\", \"MS-A_Latency\"),\n",
    "    (\"MS-B_Error\", \"MS-A_Error\"), \n",
    "    (\"MS-A_Latency\", \"MS-A_Threads\"), \n",
    "    (\"MS-A_Threads\", \"MS-A_Crash\"), \n",
    "    (\"MS-A_Error\", \"Outage\"), \n",
    "    (\"MS-A_Crash\", \"Outage\"), \n",
    "    (\"HeavyTraffic\", \"DB_Latency\"), \n",
    "    (\"HeavyTraffic\", \"MS-A_Latency\"),\n",
    "    # UNOBS\n",
    "    ('Unob_helper_1', 'DB_Change'),\n",
    "    ('Unob_helper_2', 'MS-B_Latency'),\n",
    "    ('Unob_helper_3', 'MS-B_Error'),\n",
    "    ('Unob_helper_4', 'MS-A_Error'),\n",
    "    ('Unob_helper_5', 'MS-A_Threads'),\n",
    "    ('Unob_helper_6', 'MS-A_Crash'),\n",
    "    ('Unob_helper_7', 'Outage'),\n",
    "]\n",
    "\n",
    "latent_nodes_2 = ['HeavyTraffic', 'Unob_helper_1', 'Unob_helper_2', 'Unob_helper_3', 'Unob_helper_4', 'Unob_helper_5', 'Unob_helper_6', 'Unob_helper_7']\n",
    "nodes_set_2 = set()\n",
    "for tuple in edges_list_2:\n",
    "    if tuple[0] not in latent_nodes_2:\n",
    "        nodes_set_2.add(tuple[0])\n",
    "    if tuple[1] not in latent_nodes_2:\n",
    "        nodes_set_2.add(tuple[1])\n",
    "\n",
    "edges_2 = nx.DiGraph(edges_list_2)\n",
    "\n",
    "df_medium_scale_incident = pd.read_csv(\"medium_scale_outage_incident_seed42.csv\", index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e689804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawand/.local/lib/python3.10/site-packages/dowhy/causal_model.py:582: UserWarning: 7 variables are assumed unobserved because they are not in the dataset. Configure the logging level to `logging.WARNING` or higher for additional details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mod = dowhy.CausalModel(data=df_medium_scale_incident,graph=edges_2, treatment=\"DB_Latency\", outcome=\"Outage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9066f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimand type: EstimandType.NONPARAMETRIC_ATE\n",
      "\n",
      "### Estimand : 1\n",
      "Estimand name: backdoor\n",
      "Estimand expression:\n",
      "      d                              \n",
      "─────────────(E[Outage|HeavyTraffic])\n",
      "d[DB_Latency]                        \n",
      "Estimand assumption 1, Unconfoundedness: If U→{DB_Latency} and U→Outage then P(Outage|DB_Latency,HeavyTraffic,U) = P(Outage|DB_Latency,HeavyTraffic)\n",
      "\n",
      "### Estimand : 2\n",
      "Estimand name: iv\n",
      "Estimand expression:\n",
      " ⎡                                                 -1⎤\n",
      " ⎢     d               ⎛     d                    ⎞  ⎥\n",
      "E⎢────────────(Outage)⋅⎜────────────([DB_Latency])⎟  ⎥\n",
      " ⎣d[DB_Change]         ⎝d[DB_Change]              ⎠  ⎦\n",
      "Estimand assumption 1, As-if-random: If U→→Outage then ¬(U →→{DB_Change})\n",
      "Estimand assumption 2, Exclusion: If we remove {DB_Change}→{DB_Latency}, then ¬({DB_Change}→Outage)\n",
      "\n",
      "### Estimand : 3\n",
      "Estimand name: frontdoor\n",
      "Estimand expression:\n",
      " ⎡                   d                                  ∂                      ↪\n",
      "E⎢───────────────────────────────────────(Outage)⋅─────────────([MS-A_Error  M ↪\n",
      " ⎣d[MS-A_Error  MS-B_Latency  MS-B_Error]         ∂[DB_Latency]                ↪\n",
      "\n",
      "↪                          ⎤\n",
      "↪ S-B_Latency  MS-B_Error])⎥\n",
      "↪                          ⎦\n",
      "Estimand assumption 1, Full-mediation: MS-A_Error,MS-B_Latency,MS-B_Error intercepts (blocks) all directed paths from DB_Latency to O,u,t,a,g,e.\n",
      "Estimand assumption 2, First-stage-unconfoundedness: If U→{DB_Latency} and U→{MS-A_Error,MS-B_Latency,MS-B_Error} then P(MS-A_Error,MS-B_Latency,MS-B_Error|DB_Latency,U) = P(MS-A_Error,MS-B_Latency,MS-B_Error|DB_Latency)\n",
      "Estimand assumption 3, Second-stage-unconfoundedness: If U→{MS-A_Error,MS-B_Latency,MS-B_Error} and U→Outage then P(Outage|MS-A_Error,MS-B_Latency,MS-B_Error, DB_Latency, U) = P(Outage|MS-A_Error,MS-B_Latency,MS-B_Error, DB_Latency)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Identify\n",
    "identified_estimand = mod.identify_effect(proceed_when_unidentifiable=True)\n",
    "print(identified_estimand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b36fed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Causal Estimate ***\n",
      "\n",
      "## Identified estimand\n",
      "Estimand type: EstimandType.NONPARAMETRIC_ATE\n",
      "\n",
      "### Estimand : 1\n",
      "Estimand name: backdoor\n",
      "Estimand expression:\n",
      "      d                              \n",
      "─────────────(E[Outage|HeavyTraffic])\n",
      "d[DB_Latency]                        \n",
      "Estimand assumption 1, Unconfoundedness: If U→{DB_Latency} and U→Outage then P(Outage|DB_Latency,HeavyTraffic,U) = P(Outage|DB_Latency,HeavyTraffic)\n",
      "\n",
      "## Realized estimand\n",
      "b: Outage~DB_Latency+HeavyTraffic\n",
      "Target units: ate\n",
      "\n",
      "## Estimate\n",
      "Mean value: 0.35834243000752397\n",
      "p-value: [0, 0.001]\n",
      "95.0% confidence interval: (0.32775316125581067, 0.38418893312231983)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Estimate\n",
    "# Choose the second estimand: using IV\n",
    "estimate = mod.estimate_effect(identified_estimand,\n",
    "                                 method_name=\"backdoor.propensity_score_weighting\", \n",
    "                                 test_significance=True,\n",
    "                                 confidence_intervals=True)\n",
    "\n",
    "print(estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fad2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = mod.refute_estimate(identified_estimand, \n",
    "                            estimate, \n",
    "                            method_name=\"placebo_treatment_refuter\", \n",
    "                            placebo_type=\"permute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220da25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
