{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32fcb737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import networkx as nx\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "from causal_reasoning.causal_model import CausalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5784e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, inter_label: str, inter_value: int, target_label: str, target_value: int, query_value: float) -> None:\n",
    "        self.intervention_label = inter_label\n",
    "        self.intervention_value = inter_value\n",
    "        self.target_label = target_label\n",
    "        self.target_value = target_value\n",
    "        self.query = query_value\n",
    "\n",
    "def run_queries(nodes_set: set, model: CausalModel, empirical_probs, output_path: str):\n",
    "    with open(output_path, 'w') as file:\n",
    "        queries = []\n",
    "        for intervention in nodes_set:\n",
    "            for target in nodes_set:\n",
    "                if intervention == target:\n",
    "                    continue\n",
    "                intervention_value = 0\n",
    "                target_value = 0\n",
    "                model.set_interventions([(intervention, intervention_value)])\n",
    "                model.set_target((target, target_value))\n",
    "                start = time.perf_counter()\n",
    "                pn = model.inference_intervention_query()\n",
    "                end = time.perf_counter()\n",
    "                pn_time = end-start\n",
    "                queries.append(Query(intervention, intervention_value, target, target_value, pn))\n",
    "\n",
    "\n",
    "                intervention_value = 1\n",
    "                target_value = 1\n",
    "                model.set_interventions([(intervention, intervention_value)])\n",
    "                model.set_target((target, target_value))\n",
    "                start = time.perf_counter()\n",
    "                ps = model.inference_intervention_query()\n",
    "                end = time.perf_counter()\n",
    "                ps_time = end-start\n",
    "                queries.append(Query(intervention, intervention_value, target, target_value, ps))\n",
    "                \n",
    "                # file.write(f\"PS = P({target}={target_value}|do({intervention}={intervention_value}))\\n\")\n",
    "                file.write(f\"PS = P({target}={1}|do({intervention}={1}))\\n\")\n",
    "                file.write(f\"{ps[0]} <= PS <= {ps[1]}\\n\")\n",
    "                file.write(f\"PS Inference time taken: {ps_time:.6f} seconds\\n\")\n",
    "                file.write(\"----\\n\")\n",
    "                file.write(f\"PN = P({target}={0}|do({intervention}={0}))\\n\")\n",
    "                file.write(f\"{pn[0]} <= PN <= {pn[1]}\\n\")\n",
    "                file.write(f\"PN Inference time taken: {pn_time:.6f} seconds\\n\")\n",
    "                file.write(\"----\\n\")\n",
    "\n",
    "                empirical_ps = empirical_probs[intervention][1][target]\n",
    "                empirical_pn = empirical_probs[intervention][0][target]\n",
    "                \n",
    "                if ps[0] == 'None' or ps[0] is None or ps[1] == 'None' or ps[1] is None or pn[0] == 'None' or pn[0] is None or pn[1] == 'None' or pn[1] is None: \n",
    "                    file.write(f\"PNS = P({target}|do({intervention}))\\n\")\n",
    "                    file.write(f\"PNS == None\\n\")\n",
    "                    file.write(\"---------------------------------------------------------\\n\")\n",
    "                    continue\n",
    "\n",
    "                ps_lower = empirical_ps*float(ps[0])\n",
    "                ps_upper = empirical_ps*float(ps[1])\n",
    "                pn_lower = empirical_pn*float(pn[0])\n",
    "                pn_upper = empirical_pn*float(pn[1])\n",
    "\n",
    "                pns = [0,0]\n",
    "                pns[0] = min(ps_lower+pn_lower, ps_lower+pn_upper)\n",
    "                pns[1] = max(ps_upper+pn_lower, ps_upper+pn_upper)\n",
    "                file.write(f\"PNS = P({target}|do({intervention}))\\n\")\n",
    "                file.write(f\"{pn[0]} <= PNS <= {pn[1]}\\n\")\n",
    "                file.write(\"---------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "def run_pn_ps(nodes_set: set, model: CausalModel, target: str, output_path: str):\n",
    "    pn_ps = {}\n",
    "    for intervention in nodes_set:\n",
    "        if intervention == target:\n",
    "            continue\n",
    "        pn_ps[intervention] = {}\n",
    "\n",
    "        intervention_value = 0\n",
    "        target_value = 0\n",
    "        model.set_interventions([(intervention, intervention_value)])\n",
    "        model.set_target((target, target_value))\n",
    "        start = time.perf_counter()\n",
    "        pn = model.inference_intervention_query()\n",
    "        end = time.perf_counter()\n",
    "        pn_time = end-start\n",
    "        pn_ps[intervention][\"PN\"] = pn\n",
    "\n",
    "        intervention_value = 1\n",
    "        target_value = 1\n",
    "        model.set_interventions([(intervention, intervention_value)])\n",
    "        model.set_target((target, target_value))\n",
    "        start = time.perf_counter()\n",
    "        ps = model.inference_intervention_query()\n",
    "        end = time.perf_counter()\n",
    "        ps_time = end-start\n",
    "        pn_ps[intervention][\"PS\"] = ps\n",
    "            \n",
    "    with open(f'{output_path}.json', 'w') as f:\n",
    "        json.dump(pn_ps, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa746d00",
   "metadata": {},
   "source": [
    "## First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca736e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_list_1 = [\n",
    "    ('NewDeploy', 'MemoryLeak'),\n",
    "    ('MemoryLeak', 'MemoryUsageHigh'), \n",
    "    (\"MemoryUsageHigh\", \"ServiceCrash\"), \n",
    "    (\"ServiceCrash\", \"OutageIncident\"),\n",
    "    (\"HeavyTraffic\", \"MemoryUsageHigh\"), \n",
    "    (\"HeavyTraffic\", \"ServiceCrash\"),\n",
    "    # UNOBS\n",
    "    ('Unob_helper_1', 'MemoryLeak'),\n",
    "    ('Unob_helper_2', 'OutageIncident'),\n",
    "    ('Unob_helper_3', 'NewDeploy'),\n",
    "]\n",
    "latent_nodes_1 = ['HeavyTraffic', 'Unob_helper_1', 'Unob_helper_2', 'Unob_helper_3']\n",
    "nodes_set_1 = set()\n",
    "for tuple in edges_list_1:\n",
    "    if tuple[0] not in latent_nodes_1:\n",
    "        nodes_set_1.add(tuple[0])\n",
    "    if tuple[1] not in latent_nodes_1:\n",
    "        nodes_set_1.add(tuple[1])\n",
    "\n",
    "edges_1 = nx.DiGraph(edges_list_1)\n",
    "\n",
    "df_small_scale_model = pd.read_csv(\"small_scale_outage_incident_seed42.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b3fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_probs_model_1 = {}\n",
    "for intervention in nodes_set_1:\n",
    "    empirical_probs_model_1[intervention] = [{},{}]\n",
    "    for target in nodes_set_1:\n",
    "        if target == intervention:\n",
    "            continue\n",
    "        empirical_probs_model_1[intervention][0][target] = df_small_scale_model[(df_small_scale_model[intervention] == 0) & (df_small_scale_model[target] == 0)].shape[0]\n",
    "        empirical_probs_model_1[intervention][1][target] = df_small_scale_model[(df_small_scale_model[intervention] == 1) & (df_small_scale_model[target] == 1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55bad013",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = CausalModel(\n",
    "    data=df_small_scale_model,\n",
    "    edges=edges_1,\n",
    "    unobservables_labels=latent_nodes_1,\n",
    ")\n",
    "# run_queries(nodes_set_1, model_1, empirical_probs_model_1, \"output_small_scale_outage_incident_seed42.txt\")\n",
    "run_pn_ps(nodes_set_1, model_1, \"OutageIncident\", \"output_small_scale_outage_incident_seed42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45738880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a824ea8",
   "metadata": {},
   "source": [
    "## Second Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdff8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_list_2 = [\n",
    "    (\"DB_Change\", \"DB_Latency\"),\n",
    "    (\"DB_Latency\", \"MS-B_Latency\"), \n",
    "    (\"MS-B_Latency\", \"MS-B_Error\"), \n",
    "    (\"MS-B_Latency\", \"MS-A_Latency\"),\n",
    "    (\"MS-B_Error\", \"MS-A_Error\"), \n",
    "    (\"MS-A_Latency\", \"MS-A_Threads\"), \n",
    "    (\"MS-A_Threads\", \"MS-A_Crash\"), \n",
    "    (\"MS-A_Error\", \"Outage\"), \n",
    "    (\"MS-A_Crash\", \"Outage\"), \n",
    "    (\"HeavyTraffic\", \"DB_Latency\"), \n",
    "    (\"HeavyTraffic\", \"MS-A_Latency\"),\n",
    "    # UNOBS\n",
    "    ('Unob_helper_1', 'DB_Change'),\n",
    "    ('Unob_helper_2', 'MS-B_Latency'),\n",
    "    ('Unob_helper_3', 'MS-B_Error'),\n",
    "    ('Unob_helper_4', 'MS-A_Error'),\n",
    "    ('Unob_helper_5', 'MS-A_Threads'),\n",
    "    ('Unob_helper_6', 'MS-A_Crash'),\n",
    "    ('Unob_helper_7', 'Outage'),\n",
    "]\n",
    "\n",
    "latent_nodes_2 = ['HeavyTraffic', 'Unob_helper_1', 'Unob_helper_2', 'Unob_helper_3', 'Unob_helper_4', 'Unob_helper_5', 'Unob_helper_6', 'Unob_helper_7']\n",
    "nodes_set_2 = set()\n",
    "for tuple in edges_list_2:\n",
    "    if tuple[0] not in latent_nodes_2:\n",
    "        nodes_set_2.add(tuple[0])\n",
    "    if tuple[1] not in latent_nodes_2:\n",
    "        nodes_set_2.add(tuple[1])\n",
    "\n",
    "edges_2 = nx.DiGraph(edges_list_2)\n",
    "\n",
    "df_medium_scale_incident = pd.read_csv(\"medium_scale_outage_incident_seed42.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c426e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_probs_model_2 = {}\n",
    "for intervention in nodes_set_2:\n",
    "    empirical_probs_model_2[intervention] = [{},{}]\n",
    "    for target in nodes_set_2:\n",
    "        if target == intervention:\n",
    "            continue\n",
    "        empirical_probs_model_2[intervention][0][target] = df_medium_scale_incident[(df_medium_scale_incident[intervention] == 0) & (df_medium_scale_incident[target] == 0)].shape[0]\n",
    "        empirical_probs_model_2[intervention][1][target] = df_medium_scale_incident[(df_medium_scale_incident[intervention] == 1) & (df_medium_scale_incident[target] == 1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32b8c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = CausalModel(\n",
    "    data=df_medium_scale_incident,\n",
    "    edges=edges_2,\n",
    "    unobservables_labels=latent_nodes_2,\n",
    ")\n",
    "# run_queries(nodes_set_2, model_2, empirical_probs_model_2, \"output_medium_scale_outage_incident_seed42.txt\")\n",
    "run_pn_ps(nodes_set_2, model_2, \"Outage\", \"output_medium_scale_outage_incident_seed42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0dcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-reasoning-CkLRd_GN-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
